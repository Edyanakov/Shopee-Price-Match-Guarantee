{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import math\nfrom tqdm import tqdm\nimport os\nimport pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import LabelEncoder\n\n# Visuals and CV2\nimport cv2\n\n# albumentations for augs\nimport albumentations\nfrom albumentations.pytorch.transforms import ToTensorV2\n\nfrom sklearn.model_selection import KFold, train_test_split\n\n#torch\nimport torch\nimport torch.nn as nn\nfrom torch.nn import Parameter\nfrom torch.nn import functional as F\nfrom torch.utils.data import Dataset,DataLoader\nfrom torch.optim.lr_scheduler import _LRScheduler\nfrom torch.optim import Adam, lr_scheduler\n\nimport transformers\nfrom transformers import BertTokenizer, TFBertModel\nfrom transformers import AdamW\nfrom transformers import get_linear_schedule_with_warmup,get_cosine_schedule_with_warmup\nfrom transformers import get_cosine_with_hard_restarts_schedule_with_warmup\n\nfrom cuml.neighbors import NearestNeighbors\nimport gc","metadata":{"papermill":{"duration":16.414014,"end_time":"2021-04-26T22:37:49.227279","exception":false,"start_time":"2021-04-26T22:37:32.813265","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Configuration","metadata":{"papermill":{"duration":0.015848,"end_time":"2021-04-26T22:37:49.338586","exception":false,"start_time":"2021-04-26T22:37:49.322738","status":"completed"},"tags":[]}},{"cell_type":"code","source":"NUM_WORKERS = 4\nTRAIN_BATCH_SIZE = 32\nEPOCHS = 25\nSEED = 2020\nLR = 5e-5\nGET_CV = True\n\ndevice = torch.device('cuda')\n\n################################################# MODEL ####################################################################\ntransformer_model = '../input/sentence-transformer-models/paraphrase-xlm-r-multilingual-v1/0_Transformer'\nTOKENIZER = transformers.AutoTokenizer.from_pretrained(transformer_model)\n\n################################################ Metric Loss and its params #######################################################\nloss_module = 'arcface'\ns = 30.0\nm = 0.35 \nls_eps = 0.0\neasy_margin = False\n\n############################################################################################################################\nmodel_params = {\n    'n_classes':11014,\n    'model_name':transformer_model,\n    'pooling':'clf',\n    'use_fc':False,\n    'fc_dim':512,\n    'dropout':0.0,\n    'loss_module':loss_module,\n    's':30.0,\n    'margin':0.5,\n    'ls_eps':0.0,\n    'theta_zero':0.785\n}","metadata":{"papermill":{"duration":2.159598,"end_time":"2021-04-26T22:37:51.515173","exception":false,"start_time":"2021-04-26T22:37:49.355575","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Utils","metadata":{"papermill":{"duration":0.015476,"end_time":"2021-04-26T22:37:51.547377","exception":false,"start_time":"2021-04-26T22:37:51.531901","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import random\n\ndef seed_torch(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    \nseed_torch(SEED)","metadata":{"papermill":{"duration":0.029158,"end_time":"2021-04-26T22:37:51.592346","exception":false,"start_time":"2021-04-26T22:37:51.563188","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class AverageMeter(object):\n    def __init__(self):\n        self.reset()\n    \n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n    \n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count","metadata":{"papermill":{"duration":0.026043,"end_time":"2021-04-26T22:37:51.635094","exception":false,"start_time":"2021-04-26T22:37:51.609051","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def fetch_loss():\n    loss = nn.CrossEntropyLoss(weight = normedWeights)\n    return loss","metadata":{"papermill":{"duration":0.024657,"end_time":"2021-04-26T22:37:51.676603","exception":false,"start_time":"2021-04-26T22:37:51.651946","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Dataset","metadata":{"papermill":{"duration":0.017284,"end_time":"2021-04-26T22:37:51.711322","exception":false,"start_time":"2021-04-26T22:37:51.694038","status":"completed"},"tags":[]}},{"cell_type":"code","source":"class ShopeeDataset(Dataset):\n    def __init__(self, csv):\n        self.csv = csv.reset_index()\n\n    def __len__(self):\n        return self.csv.shape[0]\n\n    def __getitem__(self, index):\n        row = self.csv.iloc[index]\n        \n        text = row.title\n        \n        text = TOKENIZER(text, padding='max_length', truncation=True, max_length=128, return_tensors=\"pt\")\n        input_ids = text['input_ids'][0]\n        attention_mask = text['attention_mask'][0]  \n        \n        return input_ids, attention_mask, torch.tensor(row.label_group)","metadata":{"papermill":{"duration":0.02684,"end_time":"2021-04-26T22:37:51.755564","exception":false,"start_time":"2021-04-26T22:37:51.728724","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def f1_score(y_true, y_pred):\n    y_true = y_true.apply(lambda x: set(x.split()))\n    y_pred = y_pred.apply(lambda x: set(x.split()))\n    intersection = np.array([len(x[0] & x[1]) for x in zip(y_true, y_pred)])\n    len_y_pred = y_pred.apply(lambda x: len(x)).values\n    len_y_true = y_true.apply(lambda x: len(x)).values\n    f1 = 2 * intersection / (len_y_pred + len_y_true)\n    return f1","metadata":{"papermill":{"duration":0.026318,"end_time":"2021-04-26T22:37:51.83327","exception":false,"start_time":"2021-04-26T22:37:51.806952","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_neighbors(df, embeddings, KNN = 50, image = True):\n    '''\n    https://www.kaggle.com/ragnar123/unsupervised-baseline-arcface?scriptVersionId=57121538\n    '''\n\n    model = NearestNeighbors(n_neighbors = KNN, metric = 'cosine')\n    model.fit(embeddings)\n    distances, indices = model.kneighbors(embeddings)\n    \n    tmp = df.groupby(['label_group'])['posting_id'].unique().to_dict()\n    df['matches'] = df['label_group'].map(tmp)\n    df['matches'] = df['matches'].apply(lambda x: ' '.join(x))\n    \n    # Iterate through different thresholds to maximize cv\n    if GET_CV:\n        if image:\n            thresholds = list(np.arange(0.1,0.7,0.05))\n        else:\n            thresholds = list(np.arange(0.6, 1.5, 0.1))\n        scores = []\n        for threshold in thresholds:\n            predictions = []\n            for k in range(embeddings.shape[0]):\n                idx = np.where(distances[k,] < threshold)[0]\n                ids = indices[k,idx]\n                #print(ids.get())\n                posting_ids = ' '.join(df['posting_id'].iloc[ids].values)\n                predictions.append(posting_ids)\n            df['pred_matches'] = predictions\n            df['f1'] = f1_score(df['matches'], df['pred_matches'])\n            score = df['f1'].mean()\n            print(f'Our f1 score for threshold {threshold} is {score}')\n            scores.append(score)\n            \n        thresholds_scores = pd.DataFrame({'thresholds': thresholds, 'scores': scores})\n        max_score = thresholds_scores[thresholds_scores['scores'] == thresholds_scores['scores'].max()]\n        best_threshold = max_score['thresholds'].values[0]\n        best_score = max_score['scores'].values[0]\n        print(f'Our best score is {best_score} and has a threshold {best_threshold}')\n        \n        # Use threshold\n        predictions = []\n        for k in range(embeddings.shape[0]):\n            if image:\n                idx = np.where(distances[k,] < 0.3)[0]\n            else:\n                idx = np.where(distances[k,] < 0.90)[0]\n            ids = indices[k,idx]\n            posting_ids = df['posting_id'].iloc[ids].values\n            predictions.append(posting_ids)\n\n    else:\n        predictions = []\n        for k in tqdm(range(embeddings.shape[0])):\n            if image:\n                idx = np.where(distances[k,] < 4.0)[0]\n            else:\n                idx = np.where(distances[k,] < 0.80)[0]\n            ids = indices[k,idx]\n            posting_ids = df['posting_id'].iloc[ids].values\n            predictions.append(posting_ids)\n        \n    del model, distances, indices\n    gc.collect()\n    return df, predictions","metadata":{"papermill":{"duration":0.037272,"end_time":"2021-04-26T22:37:51.887693","exception":false,"start_time":"2021-04-26T22:37:51.850421","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ArcMarginProduct(nn.Module):\n    r\"\"\"Implement of large margin arc distance: :\n        Args:\n            in_features: size of each input sample\n            out_features: size of each output sample\n            s: norm of input feature\n            m: margin\n            cos(theta + m)\n        \"\"\"\n    def __init__(self, in_features, out_features, s=30.0, m=0.50, easy_margin=False, ls_eps=0.0):\n        super(ArcMarginProduct, self).__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.s = s\n        self.m = m\n        self.ls_eps = ls_eps  # label smoothing\n        self.weight = Parameter(torch.FloatTensor(out_features, in_features))\n        nn.init.xavier_uniform_(self.weight)\n\n        self.easy_margin = easy_margin\n        self.cos_m = math.cos(m)\n        self.sin_m = math.sin(m)\n        self.th = math.cos(math.pi - m)\n        self.mm = math.sin(math.pi - m) * m\n\n    def forward(self, input, label):\n        # --------------------------- cos(theta) & phi(theta) ---------------------------\n        cosine = F.linear(F.normalize(input), F.normalize(self.weight))\n#         print(cosine.shape)\n#         print(label.shape)\n        sine = torch.sqrt(1.0 - torch.pow(cosine, 2))\n        phi = cosine * self.cos_m - sine * self.sin_m\n        if self.easy_margin:\n            phi = torch.where(cosine > 0, phi, cosine)\n        else:\n            phi = torch.where(cosine > self.th, phi, cosine - self.mm)\n        # --------------------------- convert label to one-hot ---------------------------\n        # one_hot = torch.zeros(cosine.size(), requires_grad=True, device='cuda')\n        one_hot = torch.zeros(cosine.size(), device='cuda')\n        one_hot.scatter_(1, label.view(-1, 1).long(), 1)\n        if self.ls_eps > 0:\n            one_hot = (1 - self.ls_eps) * one_hot + self.ls_eps / self.out_features\n        # -------------torch.where(out_i = {x_i if condition_i else y_i) -------------\n        output = (one_hot * phi) + ((1.0 - one_hot) * cosine)\n        output *= self.s\n\n        return output","metadata":{"papermill":{"duration":0.033383,"end_time":"2021-04-26T22:37:51.938525","exception":false,"start_time":"2021-04-26T22:37:51.905142","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model","metadata":{"papermill":{"duration":0.017442,"end_time":"2021-04-26T22:37:52.029068","exception":false,"start_time":"2021-04-26T22:37:52.011626","status":"completed"},"tags":[]}},{"cell_type":"code","source":"class ShopeeNet(nn.Module):\n\n    def __init__(self,\n                 n_classes,\n                 model_name='bert-base-uncased',\n                 pooling='mean_pooling',\n                 use_fc=False,\n                 fc_dim=512,\n                 dropout=0.0,\n                 loss_module='arcface',\n                 s=30.0,\n                 margin=0.50,\n                 ls_eps=0.0,\n                 theta_zero=0.785):\n        \"\"\"\n        :param n_classes:\n        :param model_name: name of model from pretrainedmodels\n            e.g. resnet50, resnext101_32x4d, pnasnet5large\n        :param pooling: One of ('SPoC', 'MAC', 'RMAC', 'GeM', 'Rpool', 'Flatten', 'CompactBilinearPooling')\n        :param loss_module: One of ('arcface', 'cosface', 'softmax')\n        \"\"\"\n        super(ShopeeNet, self).__init__()\n\n        self.transformer = transformers.AutoModel.from_pretrained(transformer_model)\n        final_in_features = self.transformer.config.hidden_size\n        \n        self.pooling = pooling\n        self.use_fc = use_fc\n    \n        if use_fc:\n            self.dropout = nn.Dropout(p=dropout)\n            self.fc = nn.Linear(final_in_features, fc_dim)\n            self.bn = nn.BatchNorm1d(fc_dim)\n            self.relu = nn.ReLU()\n            self._init_params()\n            final_in_features = fc_dim\n\n        self.loss_module = loss_module\n        if loss_module == 'arcface':\n            #self.final = ArcMarginProduct_subcenter(final_in_features, n_classes)\n            self.final = ArcMarginProduct(final_in_features, n_classes,\n                                          s=s, m=margin, easy_margin=False, ls_eps=ls_eps)\n        else:\n            self.final = nn.Linear(final_in_features, n_classes)\n\n    def _init_params(self):\n        nn.init.xavier_normal_(self.fc.weight)\n        nn.init.constant_(self.fc.bias, 0)\n        nn.init.constant_(self.bn.weight, 1)\n        nn.init.constant_(self.bn.bias, 0)\n\n    def mean_pooling(self,model_output,attention_mask):\n        token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n        sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n        sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n        return sum_embeddings / sum_mask        \n\n    def forward(self, input_ids,attention_mask, label):\n        feature = self.extract_feat(input_ids,attention_mask)\n        if self.loss_module == 'arcface':\n            logits = self.final(feature, label)\n        else:\n            logits = self.final(feature)\n        return feature, logits\n\n    def extract_feat(self, input_ids,attention_mask):\n        x = self.transformer(input_ids=input_ids,attention_mask=attention_mask)\n        \n        #features = x[0]\n        features = x.last_hidden_state[:,0,:]\n        #features = torch.mean(x[0], 1)\n        #features = self.mean_pooling(x,attention_mask)\n\n        if self.use_fc:\n            features = self.dropout(features)\n            features = self.fc(features)\n            features = self.bn(features)\n            features = self.relu(features)\n\n        return features","metadata":{"papermill":{"duration":0.03867,"end_time":"2021-04-26T22:37:52.085705","exception":false,"start_time":"2021-04-26T22:37:52.047035","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training Loop","metadata":{"papermill":{"duration":0.017682,"end_time":"2021-04-26T22:37:52.121692","exception":false,"start_time":"2021-04-26T22:37:52.10401","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def train_fn(dataloader,model,criterion,optimizer,device,scheduler,epoch):\n    model.train()\n    loss_score = AverageMeter()\n    \n    tk0 = tqdm(enumerate(dataloader), total=len(dataloader))\n    for bi,d in tk0:\n        \n        batch_size = d[0].shape[0]\n\n        input_ids = d[0]\n        attention_mask = d[1]\n        targets = d[2]\n\n        input_ids = input_ids.to(device)\n        attention_mask = attention_mask.to(device)\n        targets = targets.to(device)\n\n        optimizer.zero_grad()\n\n        _,output = model(input_ids,attention_mask,targets)\n        \n        loss = criterion(output,targets)\n        \n        loss.backward()\n        optimizer.step()\n        \n        loss_score.update(loss.detach().item(), batch_size)\n        tk0.set_postfix(Train_Loss=loss_score.avg,Epoch=epoch,LR=optimizer.param_groups[0]['lr'])\n        \n        if scheduler is not None:\n                scheduler.step()\n        \n    return loss_score","metadata":{"papermill":{"duration":0.02983,"end_time":"2021-04-26T22:37:52.1696","exception":false,"start_time":"2021-04-26T22:37:52.13977","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def eval_fn(data_loader,model,criterion,device):\n    \n    embeds = []\n    loss_score = AverageMeter()\n    \n    model.eval()\n    tk0 = tqdm(enumerate(data_loader), total=len(data_loader))\n    \n    with torch.no_grad():   \n        for bi,d in tk0:\n            batch_size = d[0].size()[0]\n\n            input_ids = d[0]\n            attention_mask = d[1]\n            targets = d[2]\n\n            input_ids = input_ids.to(device)\n            attention_mask = attention_mask.to(device)\n            targets = targets.to(device)\n\n            features,output = model(input_ids,attention_mask,targets)\n            \n            image_embeddings = features.detach().cpu().numpy()\n            embeds.append(image_embeddings)\n\n            loss = criterion(output,targets)\n            \n            loss_score.update(loss.detach().item(), batch_size)\n            tk0.set_postfix(Eval_Loss=loss_score.avg)\n            \n    image_embeddings = np.concatenate(embeds)\n    print(f'Our image embeddings shape is {image_embeddings.shape}')\n    \n    return loss_score, image_embeddings","metadata":{"papermill":{"duration":0.029764,"end_time":"2021-04-26T22:37:52.217569","exception":false,"start_time":"2021-04-26T22:37:52.187805","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Engine","metadata":{"papermill":{"duration":0.017632,"end_time":"2021-04-26T22:37:52.252934","exception":false,"start_time":"2021-04-26T22:37:52.235302","status":"completed"},"tags":[]}},{"cell_type":"code","source":"data = pd.read_csv('../input/shopee-product-matching/train.csv')\ndata['filepath'] = data['image'].apply(lambda x: os.path.join('../input/shopee-product-matching/', 'train_images', x))\nnSamples = data['label_group'].value_counts().values\nnormedWeights = [1 - (x / sum(nSamples)) for x in nSamples]\n\ndevice = torch.device(\"cuda\")\nnormedWeights = torch.FloatTensor(normedWeights).to(device)","metadata":{"papermill":{"duration":0.295614,"end_time":"2021-04-26T22:37:52.566928","exception":false,"start_time":"2021-04-26T22:37:52.271314","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import GroupKFold\n\nskf = GroupKFold(5)\ndata['fold'] = -1\nfor i, (train_idx, valid_idx) in enumerate(skf.split(X=data, groups=data['label_group'])):\n    data.loc[valid_idx, 'fold'] = i\n    \nencoder = LabelEncoder()\ndata['label_group'] = encoder.fit_transform(data['label_group'])","metadata":{"papermill":{"duration":0.103705,"end_time":"2021-04-26T22:37:52.688349","exception":false,"start_time":"2021-04-26T22:37:52.584644","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def run():\n    \n    train = data[data['fold']!=0].reset_index(drop=True)\n    valid = data[data['fold']==0].reset_index(drop=True)\n    \n    train_dataset = ShopeeDataset(\n        csv=train\n    )\n    \n    valid_dataset = ShopeeDataset(\n        csv=valid\n    )\n        \n    train_loader = torch.utils.data.DataLoader(\n        train_dataset,\n        batch_size=TRAIN_BATCH_SIZE,\n        pin_memory=True,\n        drop_last=True,\n        num_workers=NUM_WORKERS\n    )\n    \n    valid_loader = torch.utils.data.DataLoader(\n        valid_dataset,\n        batch_size=TRAIN_BATCH_SIZE,\n        pin_memory=True,\n        drop_last=False,\n        num_workers=NUM_WORKERS\n    )\n    \n    device = torch.device(\"cuda\")\n    model = ShopeeNet(**model_params)\n    model.to(device)\n    \n    \n    criterion = fetch_loss()\n    criterion.to(device)\n        \n    # Defining Optimizer with weight decay to params other than bias and layer norms\n    param_optimizer = list(model.named_parameters())\n    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n    optimizer_parameters = [\n        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.0001},\n        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0},\n            ]  \n    \n    optimizer = AdamW(optimizer_parameters, lr=LR)\n    scheduler = get_cosine_schedule_with_warmup(\n        optimizer, \n        num_warmup_steps=len(train_loader)*2, \n        num_training_steps=len(train_loader)*EPOCHS\n    )\n        \n    # THE ENGINE LOOP\n    valid_every = 1\n    best_loss = 10000\n    for epoch in range(EPOCHS):\n        train_loss = train_fn(train_loader, model,criterion, optimizer, device,scheduler=scheduler,epoch=epoch)\n        valid_loss, image_embeddings = eval_fn(valid_loader, model, criterion,device)\n        \n        if train_loss.avg < best_loss:\n            best_loss = train_loss.avg\n            torch.save(model.state_dict(),f'bert_base_best_loss_num_epochs_{EPOCHS}_{loss_module}.bin')\n            \n        if (epoch == 0) or (epoch % valid_every == 0) or (epoch == 14):\n            _,_ = get_neighbors(data, image_embeddings,  KNN=50, image = True)","metadata":{"papermill":{"duration":0.035716,"end_time":"2021-04-26T22:37:54.206958","exception":false,"start_time":"2021-04-26T22:37:54.171242","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"run()","metadata":{"papermill":{"duration":12045.925474,"end_time":"2021-04-27T01:58:40.150868","exception":false,"start_time":"2021-04-26T22:37:54.225394","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"papermill":{"duration":25.142525,"end_time":"2021-04-27T01:59:30.74719","exception":false,"start_time":"2021-04-27T01:59:05.604665","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]}]}